{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJxtl2HJ8sr3"
      },
      "source": [
        "# [S24] Introduction to Machine Learning: Bonus assignment\n",
        "**Student name:** <font color='red'>Liana Mardanova</font>\n",
        "\n",
        "**Student email:** <font color='red'>l.mardanova@innopolis.university</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YG-rK84RA-po"
      },
      "source": [
        "## Task 1. Baseline model [0pt]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGo8TIuZMdzB"
      },
      "source": [
        "Training done in Assignment 2. Model saved to \"LianaMardanova.pt\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTfpERQXMdzB"
      },
      "outputs": [],
      "source": [
        "# Import nessesary libraries\n",
        "import torch\n",
        "import os\n",
        "from torch.utils import data\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from time import time\n",
        "from tqdm import tqdm\n",
        "from torch.optim import SGD, Adam\n",
        "from torch.optim import lr_scheduler\n",
        "import pickle\n",
        "import torch\n",
        "import copy\n",
        "import torch.nn as nn\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9SOyr6nMdzC",
        "outputId": "2c2ea8d2-66e6-4244-fc76-a0c21722dbdf"
      },
      "outputs": [],
      "source": [
        "# Define device based on availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6tEOSrVMdzD",
        "outputId": "49924b2f-66c2-4058-aa52-5dd01a6547cc"
      },
      "outputs": [],
      "source": [
        "# Custom model class from assignment 2\n",
        "class CustomModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomModel, self).__init__()\n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.BatchNorm2d(num_features=16),\n",
        "            nn.Dropout(p=0.1)\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.BatchNorm2d(num_features=32),\n",
        "            nn.Dropout(p=0.15)\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.BatchNorm2d(num_features=64),\n",
        "            nn.Dropout(p=0.2)\n",
        "        )\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.BatchNorm2d(num_features=128),\n",
        "            nn.Dropout(p=0.25)\n",
        "        )\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Linear(in_features=128 * 2 * 2, out_features=512),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(num_features=512),\n",
        "            nn.Dropout(p=0.3)\n",
        "        )\n",
        "        self.fc2 = nn.Sequential(\n",
        "            nn.Linear(in_features=512, out_features=256),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(num_features=256),\n",
        "            nn.Dropout(p=0.4)\n",
        "        )\n",
        "        self.fc3 = nn.Linear(in_features=256, out_features=10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.fc3(x)\n",
        "        return F.log_softmax(x, dim=1)  # Using log softmax activation for output probabilities\n",
        "\n",
        "# Create an instance of the CustomModel and move it to the selected device\n",
        "custom_model = CustomModel().to(device)\n",
        "print(custom_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpzE8RVyMdzD",
        "outputId": "f2d6621d-ae3e-4907-ec8a-09d27fcf5bfa"
      },
      "outputs": [],
      "source": [
        "# Loading trained model from file\n",
        "custom_model_path = \"LianaMardanova.pt\"\n",
        "model_state_dict = torch.load(custom_model_path, map_location=device)\n",
        "custom_model.load_state_dict(model_state_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYJmg2Z1MdzD",
        "outputId": "11a74219-df56-44ee-ebd4-73645dd4708a"
      },
      "outputs": [],
      "source": [
        "# Define transformations for test data\n",
        "test_transforms_for_custom = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)) # Normalize the image with mean and standard deviation (as for pretrained model in next part)\n",
        "])\n",
        "\n",
        "batch_size = 128\n",
        "# Load CIFAR-10 datasets for testing\n",
        "test_dataset_for_custom = datasets.CIFAR10(root='cifar10',\n",
        "                                train=False,\n",
        "                                transform=test_transforms_for_custom,\n",
        "                                download=True)\n",
        "\n",
        "# Create data loaders for testing datasets\n",
        "test_data_loader_for_custom = data.DataLoader(test_dataset_for_custom,\n",
        "                                   batch_size=batch_size,\n",
        "                                   shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-A_M76oMdzD"
      },
      "outputs": [],
      "source": [
        "# Modified function for testing from Lab10\n",
        "def test(model, device, test_loader, criterion):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total_samples = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item()  * data.size(0)\n",
        "            total_samples += data.size(0)\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    acc = 100. * correct / len(test_loader.dataset)\n",
        "    test_loss = test_loss / total_samples\n",
        "    return test_loss, acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9l3VpDcMdzE",
        "outputId": "2c3c909f-f915-4e4a-ee6e-61113ed19eef"
      },
      "outputs": [],
      "source": [
        "# Calculate loss and accuracy of baseline model\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "test_loss, accuracy = test(custom_model, device, test_data_loader_for_custom, criterion)\n",
        "print(f\"Test loss: {test_loss}, Accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGlcbWXl9Dg-"
      },
      "source": [
        "## Task 2. Self-supervised learning [12pts]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctNxOwKdMdzE"
      },
      "source": [
        "### Part for autoencoder model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8X0oWL6MdzE"
      },
      "outputs": [],
      "source": [
        "# Define transformations for training and test data\n",
        "train_transforms_for_autoencoder = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.RandomAffine(45, shear=7, scale=(0.7, 1.3)),\n",
        "    transforms.RandomRotation(90),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))  # Normalize the image with mean and standard deviation (as in task 2 in assignment 2)\n",
        "])\n",
        "\n",
        "test_transforms_for_autoencoder = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)) # Normalize the image with mean and standard deviation (as in task 2 in assignment 2)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4W4f4HSjMdzE",
        "outputId": "83d2738b-3c37-4c47-f7da-550429e80f79"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "# Load CIFAR-10 datasets for both training and testing\n",
        "train_dataset_for_autoencoder = datasets.CIFAR10(root='cifar10',\n",
        "                                                 train=True,\n",
        "                                                 transform=train_transforms_for_autoencoder,\n",
        "                                                 download=True)\n",
        "\n",
        "test_dataset_for_autoencoder = datasets.CIFAR10(root='cifar10',\n",
        "                                                train=False,\n",
        "                                                transform=test_transforms_for_autoencoder,\n",
        "                                                download=True)\n",
        "\n",
        "# Create data loaders for training and testing datasets\n",
        "train_data_loader_for_autoencoder = data.DataLoader(train_dataset_for_autoencoder,\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    shuffle=True,\n",
        "                                                    drop_last=True)\n",
        "\n",
        "test_data_loader_for_autoencoder = data.DataLoader(test_dataset_for_autoencoder,\n",
        "                                                   batch_size=batch_size,\n",
        "                                                   shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWslQDW5MdzE",
        "outputId": "1816d849-e9d6-448e-fb28-4c6764dfcb72"
      },
      "outputs": [],
      "source": [
        "# Define the Autoencoder model (Updated version of model from https://stackoverflow.com/questions/69193892/autoencoder-for-cifar-10-with-low-accuracy)\n",
        "class AutoencoderModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AutoencoderModel, self).__init__()\n",
        "        # Encoder layers\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.BatchNorm2d(128),\n",
        "\n",
        "            nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.BatchNorm2d(64),\n",
        "\n",
        "            nn.Conv2d(64, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32)\n",
        "        )\n",
        "\n",
        "        # Decoder layers\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "            nn.BatchNorm2d(64),\n",
        "\n",
        "            nn.ConvTranspose2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "            nn.BatchNorm2d(128),\n",
        "\n",
        "            nn.Conv2d(128, 3, kernel_size=3, stride=1, padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "# Create an instance of the AutoencoderModel and move it to the selected device\n",
        "autoencoder_model = AutoencoderModel().to(device)\n",
        "print(autoencoder_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPT6pJsNMdzF"
      },
      "outputs": [],
      "source": [
        "# Same EarlyStopping class as in Assignment 2 (Task2) (Class from https://github.com/Bjarten/early-stopping-pytorch/blob/master/pytorchtools.py designed to implement early stopping functionality during training process)\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "            path (str): Path for the checkpoint to be saved to.\n",
        "                            Default: 'checkpoint.pt'\n",
        "            trace_func (function): trace print function.\n",
        "                            Default: print\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.trace_func = trace_func\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        score = -val_loss\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "        return self.early_stop\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0AhOUQFCMdzF"
      },
      "outputs": [],
      "source": [
        "# Modified functions from Lab 10 used for train and test\n",
        "def autoencoder_train(model, device, train_loader, criterion, optimizer, epoch):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    start_time = time()\n",
        "    total_samples = 0\n",
        "\n",
        "    bar = tqdm(train_loader)\n",
        "    for data, _ in bar:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, data)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item() * data.size(0)\n",
        "        total_samples += data.size(0)\n",
        "\n",
        "        bar.set_postfix({\"Loss\": format(epoch_loss/total_samples, '.6f')})\n",
        "\n",
        "    print(f'\\nTrain Epoch: {epoch}, elapsed time:{time()-start_time:.2f}s')\n",
        "    return epoch_loss / total_samples\n",
        "\n",
        "\n",
        "def autoencoder_test(model, device, test_loader, criterion):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, _ in test_loader:\n",
        "            data = data.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, data).item() * data.size(0)\n",
        "            total_samples += data.size(0)\n",
        "\n",
        "    return test_loss / total_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lr7ogAlYMdzF"
      },
      "outputs": [],
      "source": [
        "# Modified function from Lab 10 used for training\n",
        "def autoencoder_training(model, train_data_loader, test_data_loader, epochs, criterion, optimizer, early_stopping, device, save_path, train_info_path):\n",
        "    # Check if model already exists\n",
        "    if save_path and os.path.exists(save_path):\n",
        "        print(f\"File already exists at {save_path}. Model not trained, loaded from file.\")\n",
        "        model_state_dict = torch.load(save_path, map_location=device)\n",
        "        model.load_state_dict(model_state_dict)\n",
        "\n",
        "        # Load training information from file\n",
        "        with open(train_info_path, 'rb') as info_file:\n",
        "            train_losses, test_losses = pickle.load(info_file)\n",
        "\n",
        "        return train_losses, test_losses\n",
        "\n",
        "    train_losses, test_losses = [], []\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train_loss = autoencoder_train(model, device, train_data_loader, criterion, optimizer, epoch)\n",
        "        test_loss = autoencoder_test(model, device, test_data_loader, criterion)\n",
        "        # Terminate training if loss stopped to decrease\n",
        "        if early_stopping(test_loss, model):\n",
        "            print('\\nEarly stopping\\n')\n",
        "            break\n",
        "        train_losses.append(train_loss)\n",
        "        test_losses.append(test_loss)\n",
        "        print(f\"Training loss {train_loss}, test loss {test_loss}\\n\")\n",
        "\n",
        "    with open(train_info_path, 'wb') as info_file:\n",
        "        pickle.dump([train_losses, test_losses], info_file)\n",
        "\n",
        "    return train_losses, test_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKuBug9JNicE",
        "outputId": "361aea4a-fadf-47eb-f623-99d56bc82324"
      },
      "outputs": [],
      "source": [
        "# Defining parameters for training\n",
        "autoencoder_model_path = \"autoencoder_model.pt\"\n",
        "train_info_path_for_autoencoder = \"train_info_path_for_autoencoder.pkl\"\n",
        "epochs = 100\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = Adam(autoencoder_model.parameters())\n",
        "early_stopping = EarlyStopping(patience=5, delta=0.001, path=autoencoder_model_path)\n",
        "\n",
        "# Training model\n",
        "train_losses, test_losses = autoencoder_training(autoencoder_model, train_data_loader_for_autoencoder, test_data_loader_for_autoencoder, epochs, criterion, optimizer, early_stopping, device, autoencoder_model_path, train_info_path_for_autoencoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIaf_l93MdzG",
        "outputId": "af594f13-19ce-415f-e57d-7f4b35fad80f"
      },
      "outputs": [],
      "source": [
        "# Plot losses\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_losses, label='Train Loss', color='blue', linestyle='-')\n",
        "plt.plot(test_losses, label='Test Loss', color='orange', linestyle='--')\n",
        "plt.title('Train and Test Losses')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.7, linewidth=0.5, color='gray')\n",
        "plt.gca().set_facecolor('#f0f0f0')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLUxoP_PMdzG"
      },
      "source": [
        "### Part for self supervised learning model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ju5dtAl2MdzG",
        "outputId": "6423a7fd-f4dd-45d0-dd4e-8ac6251e9ebf"
      },
      "outputs": [],
      "source": [
        "# Define transformations for training and test data\n",
        "train_transforms_for_self_supervised = train_transforms_for_autoencoder\n",
        "test_transforms_for_self_supervised = test_transforms_for_autoencoder\n",
        "\n",
        "train_dataset_for_self_supervised = datasets.CIFAR10(root='cifar10', train=True, transform=train_transforms_for_self_supervised, download=True)\n",
        "test_dataset_for_self_supervised = datasets.CIFAR10(root='cifar10', train=False, transform=test_transforms_for_self_supervised, download=True)\n",
        "\n",
        "# Take 10 percent of train dataset\n",
        "_, train_indices_10_percent = train_test_split(list(range(len(train_dataset_for_self_supervised))), test_size=0.1, stratify=train_dataset_for_self_supervised.targets)\n",
        "\n",
        "# Create data loaders for training and test datasets\n",
        "train_data_loader_for_self_supervised = data.DataLoader(Subset(train_dataset_for_self_supervised, train_indices_10_percent),\n",
        "                                    batch_size=batch_size,\n",
        "                                    shuffle=True,\n",
        "                                    drop_last=True)\n",
        "\n",
        "test_data_loader_for_self_supervised = data.DataLoader(test_dataset_for_self_supervised,\n",
        "                                   batch_size=batch_size,\n",
        "                                   shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0efxUfgzMdzG",
        "outputId": "4937256d-1b58-40c4-94ce-325696053a9b"
      },
      "outputs": [],
      "source": [
        "# Define the self supervised model\n",
        "class SelfSupervisedModel(nn.Module):\n",
        "    def __init__(self, encoder):\n",
        "        super(SelfSupervisedModel, self).__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        for param in self.encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Linear(in_features=32 * 8 * 8, out_features=128),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(num_features=128),\n",
        "            nn.Dropout(p=0.4)\n",
        "        )\n",
        "        self.fc2 = nn.Sequential(\n",
        "            nn.Linear(in_features=128, out_features=64),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(num_features=64),\n",
        "            nn.Dropout(p=0.4)\n",
        "        )\n",
        "        self.fc3 = nn.Linear(in_features=64, out_features=10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = torch.flatten(x, 1)  # Flatten the input for the fully connected layers\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.fc3(x)\n",
        "        return F.log_softmax(x, dim=1)  # Using log softmax activation for output probabilities\n",
        "\n",
        "# Create an instance of the SelfSupervisedModel and move it to the selected device\n",
        "self_supervised_model = SelfSupervisedModel(autoencoder_model.encoder).to(device)\n",
        "print(self_supervised_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XTNzjt_MdzH"
      },
      "outputs": [],
      "source": [
        "# Modified functions from Lab 10 used for train and test\n",
        "def self_supervised_train(model, device, train_loader, criterion, optimizer, epoch):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    start_time = time()\n",
        "    correct = 0\n",
        "    iteration = 0\n",
        "\n",
        "    bar = tqdm(train_loader)\n",
        "    for data, target in bar:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(data)\n",
        "        # Get the index of the max log-probability\n",
        "        pred = output.argmax(dim=1, keepdim=True)\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        iteration += 1\n",
        "\n",
        "        bar.set_postfix({\"Loss\": format(epoch_loss/iteration, '.6f')})\n",
        "\n",
        "    acc = 100. * correct / len(train_loader.dataset)\n",
        "    print(f'\\nTrain Epoch: {epoch}, elapsed time:{time()-start_time:.2f}s')\n",
        "    return epoch_loss / len(train_loader.dataset), acc\n",
        "\n",
        "\n",
        "def self_supervised_test(model, device, test_loader, criterion):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item() # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    acc = 100. * correct / len(test_loader.dataset)\n",
        "    return test_loss / len(test_loader.dataset), acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMO4wjLvMdzH"
      },
      "outputs": [],
      "source": [
        "# Modified function from Lab 10 used for training\n",
        "def self_supervised_training(model, train_data_loader, test_data_loader, epochs, criterion, optimizer, early_stopping, device, save_path, train_info_path):\n",
        "    # Check if model already exists\n",
        "    if save_path and os.path.exists(save_path):\n",
        "        print(f\"File already exists at {save_path}. Model not trained, loaded from file.\")\n",
        "        model_state_dict = torch.load(save_path, map_location=device)\n",
        "        model.load_state_dict(model_state_dict)\n",
        "\n",
        "        # Load training information from file\n",
        "        with open(train_info_path, 'rb') as info_file:\n",
        "            train_accuracies, test_accuracies, train_losses, test_losses = pickle.load(info_file)\n",
        "\n",
        "        return train_accuracies, test_accuracies, train_losses, test_losses\n",
        "\n",
        "    train_accuracies, test_accuracies, train_losses, test_losses = [], [], [], []\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train_loss, train_acc = self_supervised_train(model, device, train_data_loader, criterion, optimizer, epoch)\n",
        "        test_loss, test_acc = self_supervised_test(model, device, test_data_loader, criterion)\n",
        "        # Terminate training if loss stopped to decrease\n",
        "        if early_stopping(test_loss, model):\n",
        "            print('\\nEarly stopping\\n')\n",
        "            break\n",
        "        train_accuracies.append(train_acc)\n",
        "        test_accuracies.append(test_acc)\n",
        "        train_losses.append(train_loss)\n",
        "        test_losses.append(test_loss)\n",
        "        print(f\"Training accuracy {train_acc}, test accuracy {test_acc}\")\n",
        "        print(f\"Training loss {train_loss}, test loss {test_loss}\\n\")\n",
        "\n",
        "    with open(train_info_path, 'wb') as info_file:\n",
        "        pickle.dump([train_accuracies, test_accuracies, train_losses, test_losses], info_file)\n",
        "\n",
        "    return train_accuracies, test_accuracies, train_losses, test_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoIFGAigNicF",
        "outputId": "7357c3f4-29c9-4f40-92bf-7aed7afa79e5"
      },
      "outputs": [],
      "source": [
        "# Defining parameters for training\n",
        "self_supervised_model_path = \"self_supervised_model.pt\"\n",
        "self_supervised_train_info_path = \"train_info_path_for_self_supervised.pkl\"\n",
        "epochs = 100\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = Adam(self_supervised_model.parameters())\n",
        "early_stopping = EarlyStopping(patience=10, delta=0.0001, path=self_supervised_model_path)\n",
        "\n",
        "# Training model\n",
        "train_accuracies, test_accuracies, train_losses, test_losses = self_supervised_training(self_supervised_model, train_data_loader_for_self_supervised, test_data_loader_for_self_supervised, epochs, criterion, optimizer, early_stopping, device, self_supervised_model_path, self_supervised_train_info_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4V6Jj3F7MdzH",
        "outputId": "47b9b25a-ebe6-4659-c4de-26662e63b7d2"
      },
      "outputs": [],
      "source": [
        "# Plot accuracies\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_accuracies, label='Train Accuracy', color='blue', linestyle='-')\n",
        "plt.plot(test_accuracies, label='Test Accuracy', color='orange', linestyle='--')\n",
        "plt.title('Train and Test Accuracies')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.7, linewidth=0.5, color='gray')\n",
        "plt.gca().set_facecolor('#f0f0f0')\n",
        "plt.show()\n",
        "\n",
        "# Plot losses\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_losses, label='Train Loss', color='blue', linestyle='-')\n",
        "plt.plot(test_losses, label='Test Loss', color='orange', linestyle='--')\n",
        "plt.title('Train and Test Losses')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.7, linewidth=0.5, color='gray')\n",
        "plt.gca().set_facecolor('#f0f0f0')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6d5IEMAVMdzI",
        "outputId": "66d2372e-0682-4991-9dc5-ea78a7a73121"
      },
      "outputs": [],
      "source": [
        "# Calculate loss and accuracy of the self supervised model\n",
        "test_loss, accuracy = self_supervised_test(self_supervised_model, device, test_data_loader_for_self_supervised, criterion)\n",
        "print(f\"Test loss: {test_loss}, Accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSeIRmBK9PI2"
      },
      "source": [
        "## Task 3. Auxiliary learning [12 pts]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnl7GOISMdzI"
      },
      "outputs": [],
      "source": [
        "# Assign the same data loaders for training and test datasets as for self supervised model\n",
        "train_data_loader_for_auxiliary = train_data_loader_for_self_supervised\n",
        "test_data_loader_for_auxiliary = test_data_loader_for_self_supervised"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LG3f3RYkMdzI",
        "outputId": "c810c04f-96de-494f-e9c7-6bab541f5706"
      },
      "outputs": [],
      "source": [
        "# Define the auxiliary learning model\n",
        "class AuxiliaryLearningModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AuxiliaryLearningModel, self).__init__()\n",
        "        # Encoder layers\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.BatchNorm2d(256),\n",
        "\n",
        "            nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.BatchNorm2d(128),\n",
        "\n",
        "            nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.BatchNorm2d(64),\n",
        "\n",
        "        )\n",
        "        # Decoder layers\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.BatchNorm2d(128),\n",
        "\n",
        "            nn.ConvTranspose2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.BatchNorm2d(256),\n",
        "\n",
        "            nn.Conv2d(256, 3, kernel_size=3, stride=1, padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # Classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(in_features=64 * 16 * 16, out_features=512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.4),\n",
        "            nn.BatchNorm1d(512),\n",
        "\n",
        "            nn.Linear(in_features=512, out_features=256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.3),\n",
        "            nn.BatchNorm1d(256),\n",
        "\n",
        "            nn.Linear(in_features=256, out_features=128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.35),\n",
        "            nn.BatchNorm1d(128),\n",
        "\n",
        "            nn.Linear(in_features=128, out_features=64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.4),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.Linear(in_features=64, out_features=10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass the input through the encoder\n",
        "        encoded_x = self.encoder(x)\n",
        "\n",
        "        # Pass the encoded features through the classifier for classification\n",
        "        classification_output = self.classifier(torch.flatten(encoded_x, start_dim=1))\n",
        "\n",
        "        # Pass the encoded features through the decoder for reconstruction\n",
        "        reconstructed_x = self.decoder(encoded_x)\n",
        "\n",
        "        return F.log_softmax(classification_output, dim=1), reconstructed_x\n",
        "\n",
        "# Create an instance of the AuxiliaryLearningModel and move it to the selected device\n",
        "auxiliary_model = AuxiliaryLearningModel().to(device)\n",
        "print(auxiliary_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZvy4ivwMdzI"
      },
      "outputs": [],
      "source": [
        "# Modified functions from Lab 10 used for train and test\n",
        "def auxiliary_train(model, device, train_loader, criterion_classification, criterion_reconstruction, optimizer, epoch):\n",
        "    model.train()\n",
        "    epoch_loss_classification = 0\n",
        "    epoch_loss_reconstruction = 0\n",
        "    epoch_loss_combined = 0\n",
        "    start_time = time()\n",
        "    correct = 0\n",
        "    iteration = 0\n",
        "\n",
        "    bar = tqdm(train_loader)\n",
        "    for data, target in bar:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        classification_output, reconstruction_output = model(data)\n",
        "\n",
        "        # Calculate losses\n",
        "        classification_loss = criterion_classification(classification_output, target)\n",
        "        reconstruction_loss = criterion_reconstruction(reconstruction_output, data)\n",
        "        combined_loss = 0.7 * classification_loss + 0.3 * reconstruction_loss\n",
        "\n",
        "        # Backpropagation\n",
        "        combined_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate losses and correct predictions\n",
        "        epoch_loss_classification += classification_loss.item()\n",
        "        epoch_loss_reconstruction += reconstruction_loss.item()\n",
        "        epoch_loss_combined += combined_loss.item()\n",
        "\n",
        "        pred = classification_output.argmax(dim=1, keepdim=True)\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "        iteration += 1\n",
        "\n",
        "        bar.set_postfix({\"Classification Loss\": format(epoch_loss_classification/iteration, '.6f'),\n",
        "                         \"Reconstruction Loss\": format(epoch_loss_reconstruction/iteration, '.6f'),\n",
        "                         \"Combined Loss\": format(epoch_loss_combined/iteration, '.6f')})\n",
        "\n",
        "    acc = 100. * correct / len(train_loader.dataset)\n",
        "    print(f'\\nTrain Epoch: {epoch}, elapsed time:{time()-start_time:.2f}s')\n",
        "    epoch_loss_classification /= len(train_loader.dataset)\n",
        "    epoch_loss_reconstruction /= len(train_loader.dataset)\n",
        "    epoch_loss_combined /= len(train_loader.dataset)\n",
        "\n",
        "    return epoch_loss_classification, epoch_loss_reconstruction, epoch_loss_combined, acc\n",
        "\n",
        "\n",
        "def auxiliary_test(model, device, test_loader, criterion_classification, criterion_reconstruction):\n",
        "    model.eval()\n",
        "    test_loss_classification = 0\n",
        "    test_loss_reconstruction = 0\n",
        "    test_loss_combined = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            classification_output, reconstruction_output = model(data)\n",
        "\n",
        "            # Calculate losses\n",
        "            classification_loss = criterion_classification(classification_output, target)\n",
        "            reconstruction_loss = criterion_reconstruction(reconstruction_output, data)\n",
        "            combined_loss = 0.7 * classification_loss + 0.3 * reconstruction_loss\n",
        "\n",
        "            # Accumulate losses and correct predictions\n",
        "            test_loss_classification += classification_loss.item()\n",
        "            test_loss_reconstruction += reconstruction_loss.item()\n",
        "            test_loss_combined += combined_loss.item()\n",
        "\n",
        "            pred = classification_output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    # Average losses and accuracy\n",
        "    test_loss_classification /= len(test_loader.dataset)\n",
        "    test_loss_reconstruction /= len(test_loader.dataset)\n",
        "    test_loss_combined /= len(test_loader.dataset)\n",
        "    acc = 100. * correct / len(test_loader.dataset)\n",
        "\n",
        "    return test_loss_classification, test_loss_reconstruction, test_loss_combined, acc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAPXrsJ1MdzI"
      },
      "outputs": [],
      "source": [
        "# Modified function from Lab 10 used for training\n",
        "def auxiliary_training(model, train_data_loader, test_data_loader, epochs, criterion_classification, criterion_reconstruction, optimizer, early_stopping, device, save_path, train_info_path):\n",
        "\n",
        "    # Check if model already exists\n",
        "    if save_path and os.path.exists(save_path):\n",
        "        print(f\"File already exists at {save_path}. Model not trained, loaded from file.\")\n",
        "        model_state_dict = torch.load(save_path, map_location=device)\n",
        "        model.load_state_dict(model_state_dict)\n",
        "\n",
        "        # Load training information from file\n",
        "        with open( train_info_path, 'rb') as info_file:\n",
        "            train_accuracies, test_accuracies, train_losses_classification, train_losses_reconstruction, train_losses_combined, test_losses_classification, test_losses_reconstruction, test_losses_combined  = pickle.load(info_file)\n",
        "\n",
        "        return train_accuracies, test_accuracies, train_losses_classification, train_losses_reconstruction, train_losses_combined, test_losses_classification, test_losses_reconstruction, test_losses_combined\n",
        "\n",
        "    train_accuracies, test_accuracies, train_losses_classification, train_losses_reconstruction, train_losses_combined, test_losses_classification, test_losses_reconstruction, test_losses_combined = [], [], [], [], [], [], [], []\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train_loss_cl, train_loss_re, train_loss_co, train_acc = auxiliary_train(model, device, train_data_loader, criterion_classification, criterion_reconstruction, optimizer, epoch)\n",
        "        test_loss_cl, test_loss_re, test_loss_co, test_acc = auxiliary_test(model, device, test_data_loader, criterion_classification, criterion_reconstruction)\n",
        "        # Terminate training if loss stopped to decrease\n",
        "        if early_stopping(test_loss_co, model):\n",
        "            print('\\nEarly stopping\\n')\n",
        "            break\n",
        "\n",
        "        train_accuracies.append(train_acc)\n",
        "        train_losses_classification.append(train_loss_cl)\n",
        "        train_losses_reconstruction.append(train_loss_re)\n",
        "        train_losses_combined.append(train_loss_co)\n",
        "\n",
        "        test_accuracies.append(test_acc)\n",
        "        test_losses_classification.append(test_loss_cl)\n",
        "        test_losses_reconstruction.append(test_loss_re)\n",
        "        test_losses_combined.append(test_loss_co)\n",
        "\n",
        "        print(f\"Train accuracy: {train_acc:.2f}%, Test accuracy: {test_acc:.2f}%\")\n",
        "        print(f\"Train loss (Classification): {train_loss_cl:.4f}, Train loss (Reconstruction): {train_loss_re:.4f}, Train loss (Combined): {train_loss_co:.4f}\")\n",
        "        print(f\"Test loss (Classification): {test_loss_cl:.4f}, Test loss (Reconstruction): {test_loss_re:.4f}, Test loss (Combined): {test_loss_co:.4f}\\n\")\n",
        "\n",
        "    with open(train_info_path, 'wb') as info_file:\n",
        "        pickle.dump([train_accuracies, test_accuracies, train_losses_classification, train_losses_reconstruction, train_losses_combined, test_losses_classification, test_losses_reconstruction, test_losses_combined], info_file)\n",
        "\n",
        "    return train_accuracies, test_accuracies, train_losses_classification, train_losses_reconstruction, train_losses_combined, test_losses_classification, test_losses_reconstruction, test_losses_combined"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MACG9q4DMdzJ",
        "outputId": "54858d32-af6b-4a28-e5fd-60ea882c0176"
      },
      "outputs": [],
      "source": [
        "# Defining parameters for training\n",
        "auxiliary_model_path = \"auxiliary_model.pt\"\n",
        "auxiliary_train_info_path = \"train_info_path_for_auxiliary.pkl\"\n",
        "epochs = 50\n",
        "criterion_classification = nn.CrossEntropyLoss()\n",
        "criterion_reconstruction = nn.MSELoss()\n",
        "optimizer = Adam(auxiliary_model.parameters())\n",
        "early_stopping = EarlyStopping(patience=5, path=auxiliary_model_path)\n",
        "\n",
        "# Training model\n",
        "train_accuracies, test_accuracies, train_losses_classification, train_losses_reconstruction, train_losses_combined, test_losses_classification, test_losses_reconstruction, test_losses_combined  = auxiliary_training(auxiliary_model, train_data_loader_for_auxiliary, test_data_loader_for_auxiliary, epochs, criterion_classification, criterion_reconstruction, optimizer, early_stopping, device, auxiliary_model_path, auxiliary_train_info_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1nlNE00MdzJ",
        "outputId": "f6d985b5-aab4-486f-a434-64b1981fb7b0"
      },
      "outputs": [],
      "source": [
        "# Plot accuracies\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_accuracies, label='train accuracy', color='blue', linestyle='-')\n",
        "plt.plot(test_accuracies, label='test accuracy', color='orange', linestyle='--')\n",
        "plt.title('Train and Test Accuracies')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.7, linewidth=0.5, color='gray')\n",
        "plt.gca().set_facecolor('#f0f0f0')\n",
        "plt.show()\n",
        "\n",
        "# Define beautiful colors (shades of blue for train losses and shades of orange for test losses)\n",
        "train_colors = [plt.cm.Blues(i) for i in np.linspace(0.4, 1, 3)]\n",
        "test_colors = [plt.cm.Oranges(i) for i in np.linspace(0.4, 1, 3)]\n",
        "\n",
        "# Plot losses\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_losses_classification, label='train classification loss', color=train_colors[0], linestyle='-')\n",
        "plt.plot(train_losses_reconstruction, label='train reconstruction loss', color=train_colors[1], linestyle='-')\n",
        "plt.plot(train_losses_combined, label='train combined loss', color=train_colors[2], linestyle='-')\n",
        "plt.plot(test_losses_classification, label='test classification loss', color=test_colors[0], linestyle='--')\n",
        "plt.plot(test_losses_reconstruction, label='test reconstruction loss', color=test_colors[1], linestyle='--')\n",
        "plt.plot(test_losses_combined, label='test combined loss', color=test_colors[2], linestyle='--')\n",
        "plt.title('Train and Test Losses')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.7, linewidth=0.5, color='gray')\n",
        "plt.gca().set_facecolor('#f0f0f0')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "161x-pUlMdzJ",
        "outputId": "d693cb27-f02b-4406-c681-008e46e0b48a"
      },
      "outputs": [],
      "source": [
        "test_loss_classification, test_loss_reconstruction, test_loss_combined, acc = auxiliary_test(auxiliary_model, device, test_data_loader_for_self_supervised, criterion_classification, criterion_reconstruction)\n",
        "print(f\"Test loss (Classification): {test_loss_classification:.4f}, Test loss (Reconstruction): {test_loss_reconstruction:.4f}, Test loss (Combined): {test_loss_combined:.4f}, Accuracy: {acc:.4f}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeKqSM3s9gbD"
      },
      "source": [
        "## Task 4. Ensemble [6pts]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hw9rpuh6MdzJ",
        "outputId": "865bb488-641a-4eb7-e50f-9bb6424a5b1b"
      },
      "outputs": [],
      "source": [
        "# Define class for ensemble model\n",
        "class Ensemble(nn.Module):\n",
        "    def __init__(self, base_model1, base_model2, base_model3):\n",
        "        super(Ensemble, self).__init__()\n",
        "\n",
        "        # Freeze parameters of base models\n",
        "        for param in base_model1.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in base_model2.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in base_model3.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.base_model1 = base_model1\n",
        "        self.base_model2 = base_model2\n",
        "        self.base_model3 = base_model3\n",
        "\n",
        "        self.final_layer = nn.Sequential(\n",
        "            nn.Linear(3 * 10, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.Dropout(0.5),\n",
        "\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.Dropout(0.5),\n",
        "\n",
        "            nn.Linear(64, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out1 = self.base_model1(x)\n",
        "        out2 = self.base_model2(x)\n",
        "        out3, _ = self.base_model3(x)\n",
        "        x = self.final_layer(torch.cat((out1, out2, out3), dim=1))\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "# Create an instance of the Ensemble and move it to the selected device\n",
        "ensemble_model = Ensemble(custom_model, self_supervised_model, auxiliary_model).to(device)\n",
        "print(ensemble_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O92s6g1DMdzJ"
      },
      "outputs": [],
      "source": [
        "# Modified functions from Lab 10 used for train and test\n",
        "def ensemble_train(model, device, train_loader, criterion, optimizer, epoch):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    start_time = time()\n",
        "    correct = 0\n",
        "    iteration = 0\n",
        "\n",
        "    bar = tqdm(train_loader)\n",
        "    for data, target in bar:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(data)\n",
        "        # Get the index of the max log-probability\n",
        "        pred = output.argmax(dim=1, keepdim=True)\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        iteration += 1\n",
        "\n",
        "        bar.set_postfix({\"Loss\": format(epoch_loss/iteration, '.6f')})\n",
        "\n",
        "    acc = 100. * correct / len(train_loader.dataset)\n",
        "    print(f'\\nTrain Epoch: {epoch}, elapsed time:{time()-start_time:.2f}s')\n",
        "    return epoch_loss / len(train_loader.dataset), acc\n",
        "\n",
        "\n",
        "def ensemble_test(model, device, test_loader, criterion):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item() # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    acc = 100. * correct / len(test_loader.dataset)\n",
        "    return test_loss / len(test_loader.dataset), acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2CU6zMSMdzg"
      },
      "outputs": [],
      "source": [
        "# Modified function from Lab 10 used for training\n",
        "def ensemble_training(model, train_data_loader, test_data_loader, epochs, criterion, optimizer, early_stopping, device, save_path, train_info_path):\n",
        "    # Check if model already exists\n",
        "    if save_path and os.path.exists(save_path):\n",
        "        print(f\"File already exists at {save_path}. Model not trained, loaded from file.\")\n",
        "        model_state_dict = torch.load(save_path, map_location=device)\n",
        "        model.load_state_dict(model_state_dict)\n",
        "\n",
        "        # Load training information from file\n",
        "        with open(train_info_path, 'rb') as info_file:\n",
        "            train_accuracies, test_accuracies, train_losses, test_losses = pickle.load(info_file)\n",
        "\n",
        "        return train_accuracies, test_accuracies, train_losses, test_losses\n",
        "\n",
        "    train_accuracies, test_accuracies, train_losses, test_losses = [], [], [], []\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train_loss, train_acc = ensemble_train(model, device, train_data_loader, criterion, optimizer, epoch)\n",
        "        test_loss, test_acc = ensemble_test(model, device, test_data_loader, criterion)\n",
        "        # Terminate training if loss stopped to decrease\n",
        "        if early_stopping(test_loss, model):\n",
        "            print('\\nEarly stopping\\n')\n",
        "            break\n",
        "\n",
        "        train_accuracies.append(train_acc)\n",
        "        test_accuracies.append(test_acc)\n",
        "        train_losses.append(train_loss)\n",
        "        test_losses.append(test_loss)\n",
        "\n",
        "        print(f\"Training accuracy {train_acc}, test accuracy {test_acc}\")\n",
        "        print(f\"Training loss {train_loss}, test loss {test_loss}\\n\")\n",
        "\n",
        "    with open(train_info_path, 'wb') as info_file:\n",
        "        pickle.dump([train_accuracies, test_accuracies, train_losses, test_losses], info_file)\n",
        "\n",
        "    return train_accuracies, test_accuracies, train_losses, test_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHzSU51GMdzg"
      },
      "outputs": [],
      "source": [
        "# Assing loaders from. self supervised model because they are the same\n",
        "train_data_loader_for_ensemble = train_data_loader_for_self_supervised\n",
        "test_data_loader_for_ensemble = test_data_loader_for_self_supervised"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5dFQplVMdzg",
        "outputId": "719ce60a-165c-4705-9f84-18ab2d9ecc96"
      },
      "outputs": [],
      "source": [
        "# Defining parameters for training\n",
        "ensemle_model_path = \"ensemble_model.pt\"\n",
        "ensemble_train_info_path = \"train_info_path_for_ensemble.pkl\"\n",
        "epochs = 100\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = Adam(ensemble_model.final_layer.parameters())\n",
        "early_stopping = EarlyStopping(patience=5, path=ensemle_model_path)\n",
        "\n",
        "# Training model\n",
        "train_accuracies, test_accuracies, train_losses, test_losses = ensemble_training(ensemble_model, train_data_loader_for_ensemble, test_data_loader_for_ensemble, epochs, criterion, optimizer, early_stopping, device, ensemle_model_path, ensemble_train_info_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2U9froLMdzg"
      },
      "source": [
        "### Evalutiating models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDcHjm2QPNaq",
        "outputId": "ae958801-c0a6-4c36-9a63-5715a6f3e2c3"
      },
      "outputs": [],
      "source": [
        "# !pip install torchsummary\n",
        "from torchsummary import summary\n",
        "\n",
        "# Getting summary info about models\n",
        "device = \"cpu\" # change device to avoid errors\n",
        "print(\"Base model:\")\n",
        "summary(custom_model.to(device), (3, 32, 32))\n",
        "print()\n",
        "print(\"Self-supervised model:\")\n",
        "summary(self_supervised_model.to(device),(3, 32, 32))\n",
        "print()\n",
        "print(\"Auxiliary learning model:\")\n",
        "summary(auxiliary_model.to(device), (3, 32, 32))\n",
        "\n",
        "# Summary gives error because size of ensemble model depend on other model\n",
        "print()\n",
        "print(\"Ensemble model:\")\n",
        "print(\"Ensemble model size (parameters):\", sum([param.numel() for param in ensemble_model.parameters()]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vga2nmxQMdzg",
        "outputId": "67bc8013-8540-477d-fbc9-7a1369f38251"
      },
      "outputs": [],
      "source": [
        "# Updated function from Assignment 2 to evaluate models\n",
        "def evaluate_model(model, data_loader, auxiliary=False):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    true_labels = []\n",
        "    predicted_labels = []\n",
        "    start_time = time.time()\n",
        "    total_time = 0\n",
        "    len_images = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            len_images += images.size(0)\n",
        "\n",
        "            start_time = time.time()\n",
        "            if auxiliary:\n",
        "                outputs, _ = model(images)\n",
        "            else:\n",
        "                outputs = model(images)\n",
        "            total_time += time.time() - start_time\n",
        "\n",
        "            probabilities = torch.nn.functional.softmax(outputs, dim=-1)\n",
        "            _, predictions = torch.max(probabilities, dim=1)\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "            predicted_labels.extend(predictions.cpu().numpy())\n",
        "\n",
        "    inference_time = total_time * 1000 / len_images\n",
        "\n",
        "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "\n",
        "    result_dict = {\n",
        "        \"Accuracy\": accuracy,\n",
        "        \"Inference_speed\": inference_time,\n",
        "    }\n",
        "\n",
        "    return result_dict\n",
        "\n",
        "# Evaluating models\n",
        "test_data_loader = test_data_loader_for_custom\n",
        "print(\"Base model:\")\n",
        "print(evaluate_model(custom_model, test_data_loader))\n",
        "print()\n",
        "\n",
        "print(\"Self-supervised model:\")\n",
        "print(evaluate_model(self_supervised_model, test_data_loader))\n",
        "print()\n",
        "\n",
        "print(\"Auxiliary learning model:\")\n",
        "print(evaluate_model(auxiliary_model, test_data_loader, auxiliary=True))\n",
        "print()\n",
        "\n",
        "print(\"Ensemble model:\")\n",
        "print(evaluate_model(ensemble_model, test_data_loader))\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBiP6CeEMdzh"
      },
      "source": [
        "## Results\n",
        "\n",
        "| Model Type          |  Accuracy | Labeled data | Unlabeled data | Model size (params) | Inference time (ms) |\n",
        "|---------------------|-------------------|--------------|----------------|---------------------|--------------------|\n",
        "| Baseline            | 73.52%            | 50,000       | 0              | 496,010             | 0.3418             |\n",
        "| Self-supervised     | 35.16%            | 5,000        | 50,000         | 367,850             | 1.0316             |\n",
        "| Auxiliary Learning  | 44.90%            | 5,000        | 5,000          | 9,317,773           | 6.5925             |\n",
        "| Ensemble            | 65.05%            | 50,000       | 50,000         | 10,194,891          | 7.8791             |\n",
        "\n",
        "\n",
        "- **Baseline**: The baseline model achieves the highest accuracy with a moderate amount of labeled data (50,000) and no unlabeled data. Despite its relatively small model size, it achieves very good performance. Its fast inference time is noteworthy. This superior performance could be attributed to its training on the largest labeled dataset.\n",
        "\n",
        "- **Self-supervised**: Despite utilizing a large amount of both unlabeled data (50,000) and labeled data (5,000), the self-supervised model achieves comparatively lower accuracy. This could be due to the lack of labeled data and not enough training of encoder part. While its model size is moderate, its inference time is higher than the baseline, suggesting some inefficiency.\n",
        "\n",
        "- **Auxiliary learning**: The auxiliary learning model achieves moderate accuracy with a relatively small amount of both labeled and unlabeled data (5,000 each). However, it has a large model size. Additionally, its longer inference time makes it less efficient compared to the previous models.\n",
        "\n",
        "- **Ensemble**: The ensemble model gives good accuracy. It achieves decent accuracy by leveraging models trained on substantial amounts of both labeled and unlabeled data (50,000 each), but part of ensemble model itself was trained only on a set of 5,000 labeled data. Although it has a slightly larger model size and inference time compared to the other models, it outperforms the self-supervised and auxiliary learning models in accuracy, showcasing its effectiveness in enhancing overall accuracy through pretrained models.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
